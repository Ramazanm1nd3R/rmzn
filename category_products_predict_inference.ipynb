{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a18e612",
   "metadata": {},
   "source": [
    "Two‚Äìstage ONNX inference:\n",
    "  1) category  ‚Üí 2) product-within-category\n",
    "Assumes directory layout:\n",
    "\n",
    "models/\n",
    "‚îú‚îÄ‚îÄ tokenizer/                     (save_pretrained)\n",
    "‚îú‚îÄ‚îÄ category_model/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ tokenizer/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ model_quantized.onnx\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ label_encoder.pkl\n",
    "‚îú‚îÄ‚îÄ cards/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ model_quantized.onnx\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ label_encoder.pkl\n",
    "‚îú‚îÄ‚îÄ deposits/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ ‚Ä¶\n",
    "\n",
    "–ï—Å–ª–∏ –ø–æ–¥-–º–æ–¥–µ–ª—å –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è \"<category>_common\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32631a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnxruntime\n",
      "  Downloading onnxruntime-1.22.0-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: scikit-learn in d:\\programs\\miniconda\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy in d:\\programs\\miniconda\\lib\\site-packages (1.26.4)\n",
      "Collecting coloredlogs (from onnxruntime)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: packaging in d:\\programs\\miniconda\\lib\\site-packages (from onnxruntime) (24.1)\n",
      "Requirement already satisfied: protobuf in d:\\programs\\miniconda\\lib\\site-packages (from onnxruntime) (4.25.3)\n",
      "Requirement already satisfied: sympy in d:\\programs\\miniconda\\lib\\site-packages (from onnxruntime) (1.13.2)\n",
      "Requirement already satisfied: filelock in d:\\programs\\miniconda\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.32.6-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\programs\\miniconda\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\programs\\miniconda\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in d:\\programs\\miniconda\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\programs\\miniconda\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\programs\\miniconda\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\programs\\miniconda\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\programs\\miniconda\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\programs\\miniconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\programs\\miniconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in d:\\programs\\miniconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\programs\\miniconda\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programs\\miniconda\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programs\\miniconda\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programs\\miniconda\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\programs\\miniconda\\lib\\site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime)\n",
      "  Using cached pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading onnxruntime-1.22.0-cp312-cp312-win_amd64.whl (12.7 MB)\n",
      "   ---------------------------------------- 0.0/12.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.7 MB 541.6 kB/s eta 0:00:23\n",
      "   - -------------------------------------- 0.5/12.7 MB 541.6 kB/s eta 0:00:23\n",
      "   -- ------------------------------------- 0.8/12.7 MB 559.5 kB/s eta 0:00:22\n",
      "   -- ------------------------------------- 0.8/12.7 MB 559.5 kB/s eta 0:00:22\n",
      "   --- ------------------------------------ 1.0/12.7 MB 592.2 kB/s eta 0:00:20\n",
      "   --- ------------------------------------ 1.0/12.7 MB 592.2 kB/s eta 0:00:20\n",
      "   ---- ----------------------------------- 1.3/12.7 MB 604.7 kB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 1.3/12.7 MB 604.7 kB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 1.3/12.7 MB 604.7 kB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 1.6/12.7 MB 582.5 kB/s eta 0:00:20\n",
      "   ---- ----------------------------------- 1.6/12.7 MB 582.5 kB/s eta 0:00:20\n",
      "   ----- ---------------------------------- 1.8/12.7 MB 572.0 kB/s eta 0:00:19\n",
      "   ----- ---------------------------------- 1.8/12.7 MB 572.0 kB/s eta 0:00:19\n",
      "   ----- ---------------------------------- 1.8/12.7 MB 572.0 kB/s eta 0:00:19\n",
      "   ------ --------------------------------- 2.1/12.7 MB 564.6 kB/s eta 0:00:19\n",
      "   ------ --------------------------------- 2.1/12.7 MB 564.6 kB/s eta 0:00:19\n",
      "   ------- -------------------------------- 2.4/12.7 MB 561.6 kB/s eta 0:00:19\n",
      "   ------- -------------------------------- 2.4/12.7 MB 561.6 kB/s eta 0:00:19\n",
      "   -------- ------------------------------- 2.6/12.7 MB 565.5 kB/s eta 0:00:18\n",
      "   -------- ------------------------------- 2.6/12.7 MB 565.5 kB/s eta 0:00:18\n",
      "   --------- ------------------------------ 2.9/12.7 MB 566.8 kB/s eta 0:00:18\n",
      "   --------- ------------------------------ 2.9/12.7 MB 566.8 kB/s eta 0:00:18\n",
      "   --------- ------------------------------ 2.9/12.7 MB 566.8 kB/s eta 0:00:18\n",
      "   --------- ------------------------------ 3.1/12.7 MB 555.9 kB/s eta 0:00:18\n",
      "   --------- ------------------------------ 3.1/12.7 MB 555.9 kB/s eta 0:00:18\n",
      "   --------- ------------------------------ 3.1/12.7 MB 555.9 kB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 3.4/12.7 MB 539.8 kB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 3.4/12.7 MB 539.8 kB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 3.4/12.7 MB 539.8 kB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 3.7/12.7 MB 532.0 kB/s eta 0:00:17\n",
      "   ----------- ---------------------------- 3.7/12.7 MB 532.0 kB/s eta 0:00:17\n",
      "   ----------- ---------------------------- 3.7/12.7 MB 532.0 kB/s eta 0:00:17\n",
      "   ------------ --------------------------- 3.9/12.7 MB 527.8 kB/s eta 0:00:17\n",
      "   ------------ --------------------------- 3.9/12.7 MB 527.8 kB/s eta 0:00:17\n",
      "   ------------- -------------------------- 4.2/12.7 MB 527.6 kB/s eta 0:00:17\n",
      "   ------------- -------------------------- 4.2/12.7 MB 527.6 kB/s eta 0:00:17\n",
      "   -------------- ------------------------- 4.5/12.7 MB 526.4 kB/s eta 0:00:16\n",
      "   -------------- ------------------------- 4.5/12.7 MB 526.4 kB/s eta 0:00:16\n",
      "   -------------- ------------------------- 4.5/12.7 MB 526.4 kB/s eta 0:00:16\n",
      "   -------------- ------------------------- 4.7/12.7 MB 521.4 kB/s eta 0:00:16\n",
      "   -------------- ------------------------- 4.7/12.7 MB 521.4 kB/s eta 0:00:16\n",
      "   -------------- ------------------------- 4.7/12.7 MB 521.4 kB/s eta 0:00:16\n",
      "   --------------- ------------------------ 5.0/12.7 MB 520.7 kB/s eta 0:00:15\n",
      "   --------------- ------------------------ 5.0/12.7 MB 520.7 kB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 5.2/12.7 MB 522.6 kB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 5.2/12.7 MB 522.6 kB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 5.5/12.7 MB 524.3 kB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 5.5/12.7 MB 524.3 kB/s eta 0:00:14\n",
      "   ------------------ --------------------- 5.8/12.7 MB 527.5 kB/s eta 0:00:14\n",
      "   ------------------ --------------------- 5.8/12.7 MB 527.5 kB/s eta 0:00:14\n",
      "   ------------------ --------------------- 5.8/12.7 MB 527.5 kB/s eta 0:00:14\n",
      "   ------------------- -------------------- 6.0/12.7 MB 529.6 kB/s eta 0:00:13\n",
      "   ------------------- -------------------- 6.0/12.7 MB 529.6 kB/s eta 0:00:13\n",
      "   ------------------- -------------------- 6.3/12.7 MB 533.0 kB/s eta 0:00:13\n",
      "   -------------------- ------------------- 6.6/12.7 MB 537.6 kB/s eta 0:00:12\n",
      "   -------------------- ------------------- 6.6/12.7 MB 537.6 kB/s eta 0:00:12\n",
      "   --------------------- ------------------ 6.8/12.7 MB 542.6 kB/s eta 0:00:11\n",
      "   --------------------- ------------------ 6.8/12.7 MB 542.6 kB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 7.1/12.7 MB 547.3 kB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 7.1/12.7 MB 547.3 kB/s eta 0:00:11\n",
      "   ----------------------- ---------------- 7.3/12.7 MB 549.1 kB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 7.3/12.7 MB 549.1 kB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 7.6/12.7 MB 552.7 kB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 7.6/12.7 MB 552.7 kB/s eta 0:00:10\n",
      "   ------------------------ --------------- 7.9/12.7 MB 554.8 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 7.9/12.7 MB 554.8 kB/s eta 0:00:09\n",
      "   ------------------------- -------------- 8.1/12.7 MB 558.0 kB/s eta 0:00:09\n",
      "   ------------------------- -------------- 8.1/12.7 MB 558.0 kB/s eta 0:00:09\n",
      "   ------------------------- -------------- 8.1/12.7 MB 558.0 kB/s eta 0:00:09\n",
      "   -------------------------- ------------- 8.4/12.7 MB 554.5 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 8.4/12.7 MB 554.5 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 8.4/12.7 MB 554.5 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 8.7/12.7 MB 550.6 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 8.7/12.7 MB 550.6 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 8.7/12.7 MB 550.6 kB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 8.9/12.7 MB 540.7 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 8.9/12.7 MB 540.7 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 8.9/12.7 MB 540.7 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 8.9/12.7 MB 540.7 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 8.9/12.7 MB 540.7 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 9.2/12.7 MB 522.8 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 9.2/12.7 MB 522.8 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 9.2/12.7 MB 522.8 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 9.2/12.7 MB 522.8 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 9.2/12.7 MB 522.8 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 9.4/12.7 MB 507.1 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 9.4/12.7 MB 507.1 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 9.4/12.7 MB 507.1 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 9.4/12.7 MB 507.1 kB/s eta 0:00:07\n",
      "   ------------------------------ --------- 9.7/12.7 MB 499.2 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 9.7/12.7 MB 499.2 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 9.7/12.7 MB 499.2 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 9.7/12.7 MB 499.2 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 10.0/12.7 MB 495.0 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 10.0/12.7 MB 495.0 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 10.0/12.7 MB 495.0 kB/s eta 0:00:06\n",
      "   -------------------------------- ------- 10.2/12.7 MB 493.5 kB/s eta 0:00:06\n",
      "   -------------------------------- ------- 10.2/12.7 MB 493.5 kB/s eta 0:00:06\n",
      "   --------------------------------- ------ 10.5/12.7 MB 492.7 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 10.5/12.7 MB 492.7 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 10.5/12.7 MB 492.7 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 10.7/12.7 MB 493.4 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 10.7/12.7 MB 493.4 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 11.0/12.7 MB 494.9 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 11.0/12.7 MB 494.9 kB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 11.3/12.7 MB 497.3 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 11.3/12.7 MB 497.3 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 11.5/12.7 MB 499.6 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 11.5/12.7 MB 499.6 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 11.8/12.7 MB 502.2 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 11.8/12.7 MB 502.2 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 12.1/12.7 MB 505.7 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 12.1/12.7 MB 505.7 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 12.3/12.7 MB 508.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.3/12.7 MB 508.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.7 MB 510.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.7/12.7 MB 510.0 kB/s eta 0:00:00\n",
      "Using cached transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "Downloading huggingface_hub-0.32.6-py3-none-any.whl (512 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: flatbuffers, safetensors, pyreadline3, humanfriendly, huggingface-hub, tokenizers, coloredlogs, transformers, onnxruntime\n",
      "Successfully installed coloredlogs-15.0.1 flatbuffers-25.2.10 huggingface-hub-0.32.6 humanfriendly-10.0 onnxruntime-1.22.0 pyreadline3-3.5.4 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.52.4\n"
     ]
    }
   ],
   "source": [
    "!pip install onnxruntime transformers scikit-learn numpy onnx torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62c2ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import joblib\n",
    "import onnxruntime as ort\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74ecafbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –£–∫–∞–∂–∏ –ø—É—Ç—å –¥–æ models/\n",
    "MODELS_DIR     = Path(\"models\")  # –∏–ª–∏ –ø–æ–ª–Ω—ã–π –ø—É—Ç—å\n",
    "\n",
    "TOK_DIR        = MODELS_DIR / \"tokenizer\"\n",
    "CAT_DIR        = MODELS_DIR / \"category_model\"\n",
    "CAT_TOK_DIR    = CAT_DIR / \"tokenizer\"\n",
    "CAT_MODEL_PATH = CAT_DIR / \"model_quantized.onnx\"\n",
    "CAT_LABELS     = CAT_DIR / \"label_encoder.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e37d50bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoStageClassifier:\n",
    "    def __init__(self):\n",
    "        self.cat_tok = AutoTokenizer.from_pretrained(CAT_TOK_DIR)\n",
    "        self.prod_tok = AutoTokenizer.from_pretrained(TOK_DIR)\n",
    "        self.cat_sess = ort.InferenceSession(str(CAT_MODEL_PATH))\n",
    "        self.cat_enc = joblib.load(CAT_LABELS)\n",
    "        self.prod_cache: dict[str, tuple[ort.InferenceSession, joblib]] = {}\n",
    "\n",
    "    def _run(self, sess, text, tokenizer, max_len=128):\n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"np\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_len\n",
    "        )\n",
    "        return sess.run([\"logits\"], {\n",
    "            \"input_ids\": enc[\"input_ids\"].astype(\"int64\"),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].astype(\"int64\")\n",
    "        })[0]\n",
    "\n",
    "    def predict(self, text: str) -> dict:\n",
    "        cat_logits = self._run(self.cat_sess, text, tokenizer=self.cat_tok)\n",
    "        cat_id = int(np.argmax(cat_logits, axis=1)[0])\n",
    "        category = self.cat_enc.inverse_transform([cat_id])[0]\n",
    "\n",
    "        folder = category.lower().replace(\" \", \"_\")\n",
    "        folder_path = MODELS_DIR / folder\n",
    "\n",
    "        if not folder_path.exists():\n",
    "            return {\"category\": category, \"product\": f\"{folder}_common\"}\n",
    "\n",
    "        if folder not in self.prod_cache:\n",
    "            model_path = folder_path / \"model_quantized.onnx\"\n",
    "            labels_path = folder_path / \"label_encoder.pkl\"\n",
    "            if not model_path.exists() or not labels_path.exists():\n",
    "                return {\"category\": category, \"product\": f\"{folder}_common\"}\n",
    "            sess = ort.InferenceSession(str(model_path))\n",
    "            enc = joblib.load(labels_path)\n",
    "            self.prod_cache[folder] = (sess, enc)\n",
    "\n",
    "        sess, enc = self.prod_cache[folder]\n",
    "        prod_logits = self._run(sess, text, tokenizer=self.prod_tok)\n",
    "        prod_id = int(np.argmax(prod_logits, axis=1)[0])\n",
    "        product = enc.inverse_transform([prod_id])[0]\n",
    "\n",
    "        return {\"category\": category, \"product\": product}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e6f66d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\MiniConda\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.6.1 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clf = TwoStageClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2397d80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ –∫–∞–∫ –∑–∞–∫–∞–∑–∞—Ç—å –∫–∞—Ä—Ç—É?\n",
      "‚Üí {'category': 'Cards', 'product': 'cards_common'}\n",
      "\n",
      "üü¢ –º–æ–∂–Ω–æ –ª–∏ –æ—Ç–∫—Ä—ã—Ç—å –≤–∫–ª–∞–¥ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏?\n",
      "‚Üí {'category': 'Deposits', 'product': 'deposits_common'}\n",
      "\n",
      "üü¢ —Ö–æ—á—É –æ–ø–ª–∞—Ç–∏—Ç—å —à—Ç—Ä–∞—Ñ\n",
      "‚Üí {'category': 'Other', 'product': 'operator'}\n",
      "\n",
      "üü¢ –†–∞–±–æ—Ç–∞–µ—Ç –ª–∏ QR –æ–ø–ª–∞—Ç–∞ –¥–ª—è –∫–æ–º–º—É–Ω–∞–ª—å–Ω—ã—Ö —É—Å–ª—É–≥?\n",
      "‚Üí {'category': 'Payments', 'product': 'payments_common'}\n",
      "\n",
      "üü¢ –º–æ–∂–Ω–æ –ª–∏ –≤–∑—è—Ç—å –∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç?\n",
      "‚Üí {'category': 'Avtokredit', 'product': 'avtokredit_common'}\n",
      "\n",
      "üü¢ –∫–∞–∫ –≤–∑—è—Ç—å –∫—Ä–µ–¥–∏—Ç –ø–æ–¥ –∑–∞–ª–æ–≥ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏\n",
      "‚Üí {'category': 'Zalogovoe', 'product': 'zalog_nedvizhimosti'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    \"–∫–∞–∫ –∑–∞–∫–∞–∑–∞—Ç—å –∫–∞—Ä—Ç—É?\",\n",
    "    \"–º–æ–∂–Ω–æ –ª–∏ –æ—Ç–∫—Ä—ã—Ç—å –≤–∫–ª–∞–¥ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏?\",\n",
    "    \"—Ö–æ—á—É –æ–ø–ª–∞—Ç–∏—Ç—å —à—Ç—Ä–∞—Ñ\",\n",
    "    \"–†–∞–±–æ—Ç–∞–µ—Ç –ª–∏ QR –æ–ø–ª–∞—Ç–∞ –¥–ª—è –∫–æ–º–º—É–Ω–∞–ª—å–Ω—ã—Ö —É—Å–ª—É–≥?\",\n",
    "    \"–º–æ–∂–Ω–æ –ª–∏ –≤–∑—è—Ç—å –∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç?\",\n",
    "    \"–∫–∞–∫ –≤–∑—è—Ç—å –∫—Ä–µ–¥–∏—Ç –ø–æ–¥ –∑–∞–ª–æ–≥ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏\"\n",
    "]\n",
    "\n",
    "for text in examples:\n",
    "    result = clf.predict(text)\n",
    "    print(f\"üü¢ {text}\\n‚Üí {result}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524623b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference.py\n",
    "\n",
    "from pathlib import Path\n",
    "import argparse, joblib, numpy as np, onnxruntime as ort\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# ----------------- –ü–£–¢–ò –ö –§–ê–ô–õ–ê–ú -----------------\n",
    "MODELS_DIR     = Path(__file__).resolve().parent / \"models\"\n",
    "TOK_DIR        = MODELS_DIR / \"tokenizer\"                      # –æ–±—â–∏–π –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤\n",
    "CAT_DIR        = MODELS_DIR / \"category_model\"\n",
    "CAT_TOK_DIR    = CAT_DIR / \"tokenizer\"                         # —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
    "CAT_MODEL_PATH = CAT_DIR / \"model_quantized.onnx\"\n",
    "CAT_LABELS     = CAT_DIR / \"label_encoder.pkl\"\n",
    "\n",
    "\n",
    "class TwoStageClassifier:\n",
    "    def __init__(self):\n",
    "        # üîπ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã\n",
    "        self.cat_tok = AutoTokenizer.from_pretrained(CAT_TOK_DIR)  # –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è\n",
    "        self.prod_tok = AutoTokenizer.from_pretrained(TOK_DIR)     # –ø—Ä–æ–¥—É–∫—Ç—ã\n",
    "\n",
    "        # üîπ ONNX —Å–µ—Å—Å–∏—è + LabelEncoder (–∫–∞—Ç–µ–≥–æ—Ä–∏–∏)\n",
    "        self.cat_sess = ort.InferenceSession(str(CAT_MODEL_PATH))\n",
    "        self.cat_enc = joblib.load(CAT_LABELS)\n",
    "\n",
    "        # üîπ –∫–µ—à {folder ‚Üí (session, encoder)}\n",
    "        self.prod_cache: dict[str, tuple[ort.InferenceSession, joblib]] = {}\n",
    "\n",
    "    def _run(self, sess, text, tokenizer, max_len=128):\n",
    "        enc = tokenizer(text, return_tensors=\"np\", padding=\"max_length\",\n",
    "                        truncation=True, max_length=max_len)\n",
    "        return sess.run([\"logits\"], {\n",
    "            \"input_ids\": enc[\"input_ids\"],\n",
    "            \"attention_mask\": enc[\"attention_mask\"]\n",
    "        })[0]\n",
    "\n",
    "    def predict(self, text: str) -> dict:\n",
    "        # === ‚ë† –ö–∞—Ç–µ–≥–æ—Ä–∏—è ===\n",
    "        cat_logits = self._run(self.cat_sess, text, tokenizer=self.cat_tok)\n",
    "        cat_id = int(np.argmax(cat_logits, axis=1)[0])\n",
    "        category = self.cat_enc.inverse_transform([cat_id])[0]\n",
    "\n",
    "        folder = category.lower().replace(\" \", \"_\")\n",
    "        folder_path = MODELS_DIR / folder\n",
    "\n",
    "        # === ‚ë° –ü—Ä–æ–¥—É–∫—Ç ===\n",
    "        if not folder_path.exists():\n",
    "            return {\"category\": category, \"product\": f\"{folder}_common\"}\n",
    "\n",
    "        if folder not in self.prod_cache:\n",
    "            model_path = folder_path / \"model_quantized.onnx\"\n",
    "            labels_path = folder_path / \"label_encoder.pkl\"\n",
    "            if not model_path.exists() or not labels_path.exists():\n",
    "                return {\"category\": category, \"product\": f\"{folder}_common\"}\n",
    "\n",
    "            sess = ort.InferenceSession(str(model_path))\n",
    "            enc = joblib.load(labels_path)\n",
    "            self.prod_cache[folder] = (sess, enc)\n",
    "\n",
    "        sess, enc = self.prod_cache[folder]\n",
    "        prod_logits = self._run(sess, text, tokenizer=self.prod_tok)\n",
    "        prod_id = int(np.argmax(prod_logits, axis=1)[0])\n",
    "        product = enc.inverse_transform([prod_id])[0]\n",
    "\n",
    "        return {\"category\": category, \"product\": product}\n",
    "\n",
    "\n",
    "# ----------------- CLI –∑–∞–ø—É—Å–∫ -----------------\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Two-stage ONNX classifier\")\n",
    "    parser.add_argument(\"text\", nargs=\"*\", help=\"Input query\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.text:\n",
    "        query = \" \".join(args.text)\n",
    "    else:\n",
    "        query = input(\"–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å: \")\n",
    "\n",
    "    print(f\"[INFO] –í–≤–æ–¥: {query}\")\n",
    "\n",
    "    clf = TwoStageClassifier()\n",
    "    result = clf.predict(query)\n",
    "\n",
    "    if not result:\n",
    "        print(\"[WARNING] –ú–æ–¥–µ–ª—å –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç\")\n",
    "    else:\n",
    "        print(\"[RESULT]\", result)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
